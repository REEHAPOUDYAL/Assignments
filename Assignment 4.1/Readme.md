# LLM Prompting Techniques for Text Analysis
This repository contains my exploration and implementation of various prompting techniques for Large Language Models (LLMs), specifically applied to text summarization and sentiment analysis tasks. The project aims to demonstrate how different prompting strategies can influence the quality and detail of LLM outputs.

# Project Overview
In this project, I experimented with three key prompting techniques: Direct Prompting, Few-Shot Prompting, and Chain-of-Thought (CoT) Prompting. I applied these techniques to two common NLP tasks: summarizing a news article and analyzing the sentiment of a customer review. My goal was to observe and compare the responses generated by a Gemini model under each prompting condition.

# Key Concepts & My Implementation
Large Language Models (LLMs): I utilized Google's Gemini 1.5 Flash model via the google-generativeai library to perform text generation and analysis tasks.

# Prompt Engineering: The core of this project involved crafting different prompts to guide the LLM's behavior and output.

Direct Prompting: This is the most straightforward approach, where the instruction is given directly to the LLM without examples or step-by-step guidance. I used this for both summarization and sentiment analysis as a baseline.

Few-Shot Prompting: This technique involves providing the LLM with a few examples of input-output pairs before giving the actual task. I implemented this for sentiment analysis to see if providing examples improved the model's ability to classify sentiment correctly.

Chain-of-Thought (CoT) Prompting: This advanced technique encourages the LLM to "think step-by-step" before providing a final answer. I applied CoT to both summarization and sentiment analysis to observe if explicit reasoning steps led to more detailed or accurate outputs. For sentiment analysis, it involved breaking down the task into identifying positive phrases, negative phrases, neutral points, and then determining overall sentiment.

Text Summarization: I applied the prompting techniques to summarize a news article, evaluating how different prompts affect the conciseness and content of the summary.

Sentiment Analysis: I used the prompting techniques to analyze the sentiment of a customer review, comparing how each prompt influences the model's ability to correctly identify the sentiment (Positive, Negative, or Neutral) and provide reasoning.

Data Handling: I used pandas to load and process a CSV file (True.csv), extracting an article and its title/source for the summarization task.

# Analysis & Observations
Summarization:

Direct Prompt: Expected to provide a concise summary directly.

Chain-of-Thought Prompt: Encourages the model to first outline key facts and implications, leading to a more structured and potentially more comprehensive summary.

Sentiment Analysis:

Direct Prompt: Provides a quick, single-word sentiment (e.g., "Negative" for the example review).

Few-Shot Prompt: By providing examples, the model learns the desired output format and classification style, which can improve consistency and accuracy for similar inputs. For the given example, it correctly classified "Negative."

Chain-of-Thought Prompt: This was particularly insightful. The model broke down the review into positive and negative phrases, identified mixed points, and then logically concluded the overall sentiment. This detailed reasoning process makes the output more transparent and verifiable, even if the final sentiment classification (e.g., "Negative" for the example review) is the same as the direct/few-shot methods. It helps in understanding why the model arrived at that sentiment.

Overall, my experiments demonstrated that while LLMs can perform tasks with direct prompts, Few-Shot and especially Chain-of-Thought prompting can significantly enhance the quality, structure, and interpretability of the generated outputs by guiding the model's reasoning process.
